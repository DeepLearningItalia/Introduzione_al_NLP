{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will use functions from the [Scikit-learn](https://scikit-learn.org/stable/index.html) library to vectorize sentences with different methods.\n",
        "\n",
        "The sentences are taken from the [Parallel Meaning Bank](https://pmb.let.rug.nl/), a multilingual corpus of public domain text annotated with many features. First, let's download the corpus and extract the content of the archive. We use version 1.0.0 because of its reduced size of about 11MB (the following versions are much larger)."
      ],
      "metadata": {
        "id": "xHJNg5ayWxSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://pmb.let.rug.nl/releases/pmb-1.0.0.zip\n",
        "!unzip pmb-1.0.0.zip"
      ],
      "metadata": {
        "id": "ua1UwMlLeeji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `data' directory contains several subdirectories (p* , for parts). In each p directory, there are several documents, for each directory d* (document).\n",
        "\n",
        "We will read only the files called en.tok.off, containing the tokenized text in English, one token per line."
      ],
      "metadata": {
        "id": "8ZRkqngYXk_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "corpus = []\n",
        "for filename in glob(\"pmb-1.0.0/data/p*/d*/en.tok.off\"):\n",
        "    with open(filename) as f:\n",
        "        lines = f.readlines()\n",
        "        tokens = []\n",
        "        for line in lines:\n",
        "            tokens.append(\" \".join(line.strip().split(\" \")[3:]))\n",
        "        corpus.append(\" \".join(tokens))"
      ],
      "metadata": {
        "id": "8tmxfoiXfqX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end of the loop, the variable *corpus* contains a list of documents, each represented as a list of tokens."
      ],
      "metadata": {
        "id": "otQP-J2IYOge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (corpus[0:2])"
      ],
      "metadata": {
        "id": "6HpZmnW8YXj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-learn (in short, *sklearn*) provides useful functions for vectorization.A CountVectorizer object computes a vocabulary given a corpus (fit) and vectorizes sentences by counting word occurrences.\n",
        "The result is returned as a Numpy 2-dimensional sparse array."
      ],
      "metadata": {
        "id": "0NxamCOiYg1a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ui0NTUCBFbe"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(max_features=1000)\n",
        "corpus_vectorized = cv.fit_transform(corpus)\n",
        "print(corpus_vectorized[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TfidfVectorizer works similarly to CountVectorizer, but it also weights the features by their TF-IDF."
      ],
      "metadata": {
        "id": "gOpG8uboZ-SV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tv = TfidfVectorizer(max_features=1000)\n",
        "corpus_vectorized = tv.fit_transform(corpus)\n",
        "print(corpus_vectorized[0])"
      ],
      "metadata": {
        "id": "N_ZadyIFWsOs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}