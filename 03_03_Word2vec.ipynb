{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will train and test a word embedding model with Word2vec.\n",
        "\n",
        "The model is trained on a sample corpus of English texts from Wikipedia.\n",
        "In the first block, let's dowload the dataset in XML format and parse it to extract only the text of the short abstracts of each article."
      ],
      "metadata": {
        "id": "9H_YbB235w6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dumps.wikimedia.org/enwiki/20220820/enwiki-20220820-abstract1.xml.gz\n",
        "\n",
        "import gzip\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "with gzip.open('enwiki-20220820-abstract1.xml.gz') as xml_file:\n",
        "    tree = ET.parse(xml_file)\n",
        "root = tree.getroot()\n",
        "\n",
        "documents = []\n",
        "for doc in root:\n",
        "    abstract = doc.find(\"abstract\")\n",
        "    if abstract.text and len(abstract.text)>100:\n",
        "        documents.append(abstract.text)\n",
        "print (len(documents))"
      ],
      "metadata": {
        "id": "MPtNg-i8dji7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to train the embeddings with sentences rather than documents, so we tokenize the texts with the [default tokenizer](https://www.nltk.org/api/nltk.tokenize.html) for English from [NLTK](https://www.nltk.org/).\n",
        "\n",
        "We also apply some basic normalization, i.e., putting all the words in lowercase.\n"
      ],
      "metadata": {
        "id": "PnhXkUa86lSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sentences = []\n",
        "for document in documents:\n",
        "    words = [word.lower() for word in word_tokenize(document)]\n",
        "    sentences.append(words)\n",
        "        "
      ],
      "metadata": {
        "id": "_l6x_Koikx3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a Word2vec model with Gensim is straightforward. Check the [official documentation](https://www.nltk.org/api/nltk.tokenize.html) to explore the options and hyperparameters."
      ],
      "metadata": {
        "id": "1AJBqqKl7dae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(\n",
        "    sentences=sentences, \n",
        "    size=100, \n",
        "    window=5, \n",
        "    min_count=1, \n",
        "    workers=4)\n",
        "\n",
        "model.save(\"word2vec.model\")"
      ],
      "metadata": {
        "id": "mOtJgXkLQsiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the word embedding model is trained, a dictionary (\"vw\" for word vector) becomes available that contains a vector representation for each work in the vocabulary.\n",
        "\n",
        "The embeddings are ndarray objects from [Numpy](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html)."
      ],
      "metadata": {
        "id": "C337C9Ju7v92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (type(model.wv['rock']))\n",
        "print (model.wv['rock'])"
      ],
      "metadata": {
        "id": "MSeryQA97v0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gensim offers useful functions to manipulate word embeddings, such as finding the N words whose representations are closer in the geometric space to an input word."
      ],
      "metadata": {
        "id": "-932o40U7vfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word, similarity in model.wv.most_similar('rock', topn=10):\n",
        "    print (f\"{similarity:.2f} {word}\")\n"
      ],
      "metadata": {
        "id": "pXdBxZk_RGA-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
