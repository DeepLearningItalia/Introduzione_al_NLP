{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will use functions from the [Scikit-learn](https://scikit-learn.org/stable/index.html) library to create a word-document matrix and compute the Singular Value Decomposition to implement Latent Semantic Analysis.\n",
        "\n",
        "The sentences, already tokenized, are taken from the [Parallel Meaning Bank](https://pmb.let.rug.nl/) version 1.0.0."
      ],
      "metadata": {
        "id": "xHJNg5ayWxSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://pmb.let.rug.nl/releases/pmb-1.0.0.zip\n",
        "!unzip pmb-1.0.0.zip\n",
        "\n",
        "from glob import glob\n",
        "corpus = []\n",
        "for filename in glob(\"pmb-1.0.0/data/p*/d*/en.tok.off\"):\n",
        "    with open(filename) as f:\n",
        "        lines = f.readlines()\n",
        "        tokens = []\n",
        "        for line in lines:\n",
        "            tokens.append(\" \".join(line.strip().split(\" \")[3:]))\n",
        "        corpus.append(\" \".join(tokens))"
      ],
      "metadata": {
        "id": "ua1UwMlLeeji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A word-document matrix (A) is just the transpose of the result of Sklearn's count vectorization."
      ],
      "metadata": {
        "id": "0NxamCOiYg1a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ui0NTUCBFbe"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer()\n",
        "corpus_vectorized = cv.fit_transform(corpus)\n",
        "A = corpus_vectorized.T"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also saves the vocabulary computed by the CountVectorizer object as a list. We will use it later to retrieve the vector representation of the words."
      ],
      "metadata": {
        "id": "bfKhlXdzg_2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = list(cv.get_feature_names_out())"
      ],
      "metadata": {
        "id": "OllTgU5agjwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a utility function to compute the cosine similarity of two vectors given two words and a vector space model."
      ],
      "metadata": {
        "id": "yLL5nhwRhWpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def similarity(model, word1, word2):\n",
        "    vector1 = model[vocabulary.index(word1)]\n",
        "    vector2 = model[vocabulary.index(word2)]\n",
        "    return cosine_similarity(vector1, vector2)[0][0]"
      ],
      "metadata": {
        "id": "cDPWJegubCyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The word-document matrix is a very sparse vector model which suffer from the *curse of dimensionality*, as shown by the many zeros returned as similarity scores."
      ],
      "metadata": {
        "id": "gltNyMKqhiOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (similarity(A, 'cat', 'dog'))\n",
        "print (similarity(A, 'cat', 'man'))\n",
        "print (similarity(A, 'cat', 'car'))\n",
        "print (similarity(A, 'cat', 'plane'))"
      ],
      "metadata": {
        "id": "75_6KfPnhibl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TruncatedSVD function computes the Singular Value Decomposition, truncates the components to the top N, and returns their product, representing the best approximation of the original matrix as per the Latent Semantic Analysis algorithm."
      ],
      "metadata": {
        "id": "gOpG8uboZ-SV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "import scipy\n",
        "svd =  TruncatedSVD(n_components = 50)\n",
        "A_transformed = scipy.sparse.csc_matrix(svd.fit_transform(A))"
      ],
      "metadata": {
        "id": "N_ZadyIFWsOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LSA matrix is dense and provides more reasonable similarity scores."
      ],
      "metadata": {
        "id": "pwlUqUS3iTzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (similarity(A_transformed, 'cat', 'dog'))\n",
        "print (similarity(A_transformed, 'cat', 'man'))\n",
        "print (similarity(A_transformed, 'cat', 'car'))\n",
        "print (similarity(A_transformed, 'cat', 'plane'))"
      ],
      "metadata": {
        "id": "JXuO68YNdAaT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}