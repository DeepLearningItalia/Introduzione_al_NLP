{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will implement a simple neural network to classify texts.\n",
        "\n",
        "The dataset we will use is Offensive Language Identification (OLID), where short texts in English are labeled for offensiveness. We focus on subtask A: binary classification of offensiveness."
      ],
      "metadata": {
        "id": "YTKyjcXGmBHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://sites.google.com/site/offensevalsharedtask/olid/OLIDv1.0.zip\n",
        "!unzip OLIDv1.0.zip"
      ],
      "metadata": {
        "id": "OsfrOamEWebM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "data_train = []\n",
        "labels_train = []\n",
        "\n",
        "with open(\"olid-training-v1.0.tsv\") as f:\n",
        "    reader = csv.DictReader(f, delimiter=\"\\t\")\n",
        "    for row in reader:\n",
        "        words = [word.lower() for word in word_tokenize(row[\"tweet\"])]\n",
        "        data_train.append(words)\n",
        "        labels_train.append(row[\"subtask_a\"])\n",
        "\n",
        "data_test = []\n",
        "labels_test = []\n",
        "with open(\"testset-levela.tsv\") as f:\n",
        "    reader = csv.DictReader(f, delimiter=\"\\t\")\n",
        "    for row in reader:\n",
        "        words = [word.lower() for word in word_tokenize(row[\"tweet\"])]\n",
        "        data_test.append(words)\n",
        "\n",
        "with open(\"labels-levela.csv\") as f:\n",
        "    reader = csv.DictReader(f, fieldnames=[\"id\", \"label\"])\n",
        "    for row in reader:\n",
        "        labels_test.append(row[\"label\"])\n"
      ],
      "metadata": {
        "id": "yHd0sKA2XwyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use Keras' tokenizer only to compute the vocabulary on the training set. Sentences are truncated at 100 tokens and padding is added for shortes sentences."
      ],
      "metadata": {
        "id": "JaUGuWVzpN6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# transform the sentences into vectors\n",
        "tokenizer = Tokenizer(filters='', lower=True, split=' ')\n",
        "tokenizer.fit_on_texts(data_train)\n",
        "word_index = tokenizer.word_index\n",
        "X_train = tokenizer.texts_to_matrix(data_train)\n",
        "X_train = pad_sequences(X_train, 100, padding='post', truncating='post')\n",
        "\n",
        "# encode the labels\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(labels_train)\n",
        "y_train = encoder.transform(labels_train)\n",
        "\n",
        "# vectorize the test set\n",
        "X_test = tokenizer.texts_to_matrix(data_test)\n",
        "X_test = pad_sequences(X_test, 100, padding='post', truncating='post')\n",
        "y_test = encoder.transform(labels_test)\n"
      ],
      "metadata": {
        "id": "txQLn6TrVFjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The neural network has a first layer where the embeddings are input. They are then concatenated by the Flatten layer and passed on a smaller fully connected hidden layer. The output layer is one neuron with sigmoid activation for binary classification (offensive/not offensive)."
      ],
      "metadata": {
        "id": "Ga5Ph1p1pymp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S40nnahGU_82"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten\n",
        "import tensorflow as tf\n",
        "\n",
        "# create the models\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index)+1, 300, input_shape=(100,)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, input_shape=(X_train.shape[1],)))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                  metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                        batch_size=16,\n",
        "                        epochs=10,\n",
        "                        shuffle=True,\n",
        "                        validation_split=0.1,\n",
        "                        verbose=1\n",
        "                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-learn has useful functions to provide evaluation metrics as precision, recall and F1-score."
      ],
      "metadata": {
        "id": "pujzoSZYqBrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "pred = [int(x>=0.5) for x in model.predict(X_test)]\n",
        "print (classification_report(y_test, pred))"
      ],
      "metadata": {
        "id": "qgYZiDUScZW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to initialize the weights of the first layer with pre-trained embeddings from GloVe."
      ],
      "metadata": {
        "id": "SZ_cfMoRqFS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "import numpy as np\n",
        "\n",
        "glove2word2vec(\"glove.6B.300d.txt\", \"glove_gensim.6B.300d.txt\")\n",
        "embedding_model=KeyedVectors.load_word2vec_format(\"glove_gensim.6B.300d.txt\",binary=False)"
      ],
      "metadata": {
        "id": "nPZIGPnZgDk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    try:\n",
        "        embedding_vector = embedding_model[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        continue\n"
      ],
      "metadata": {
        "id": "JhSDaCK7iMLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embedding layer can be set to be trainable, or the weights can be kept frozen."
      ],
      "metadata": {
        "id": "yd48OgjuqPzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index)+1, 300, input_shape=(100,), weights=[embedding_matrix], trainable=True))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, input_shape=(X_train.shape[1],)))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                  metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                        batch_size=16,\n",
        "                        epochs=10,\n",
        "                        shuffle=True,\n",
        "                        validation_split=0.1,\n",
        "                        verbose=1\n",
        "                        )"
      ],
      "metadata": {
        "id": "hxv1cnNIiOm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = [int(x>=0.5) for x in model.predict(X_test)]\n",
        "print (classification_report(y_test, pred))"
      ],
      "metadata": {
        "id": "v1CVwCKnjfgQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
