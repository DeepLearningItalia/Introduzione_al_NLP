{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will implement a Long Short-term Memory (LSTM) network to classify texts.\n",
        "\n",
        "The dataset we will use is Offensive Language Identification (OLID), where short texts in English are labeled for offensiveness. We focus on subtask A: binary classification of offensiveness."
      ],
      "metadata": {
        "id": "YTKyjcXGmBHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://sites.google.com/site/offensevalsharedtask/olid/OLIDv1.0.zip\n",
        "!unzip OLIDv1.0.zip"
      ],
      "metadata": {
        "id": "OsfrOamEWebM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd082e48-0811-4f51-ddb2-15069326ccd9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-21 15:22:18--  https://sites.google.com/site/offensevalsharedtask/olid/OLIDv1.0.zip\n",
            "Resolving sites.google.com (sites.google.com)... 209.85.145.139, 209.85.145.101, 209.85.145.102, ...\n",
            "Connecting to sites.google.com (sites.google.com)|209.85.145.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://sites.google.com/site/offensevalsharedtask/olid/OLIDv1.0.zip?attredirects=0 [following]\n",
            "--2022-10-21 15:22:18--  https://sites.google.com/site/offensevalsharedtask/olid/OLIDv1.0.zip?attredirects=0\n",
            "Reusing existing connection to sites.google.com:443.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://ef80a887-a-62cb3a1a-s-sites.googlegroups.com/site/offensevalsharedtask/olid/OLIDv1.0.zip?attachauth=ANoY7cqEQR0eJ5TLdEsK4S1HzQw5bFGFDWy-0jht98Ph-6Q8mGDuWzno3ilDDXtWlo5Evm7FTuUwxwnaoPtcYIEaqxaqZj1BEJtqIPnfqS07Be3tEBGitu-JEuLBcEelaRVgKD6AQbJHTqClaV1TYKCyEd-vQmm7aDz66FGYfal_tS623Ld5T92DKYesJ21oZcOKVMBPe8rNZysr3OnhGI5A9LptINDjoZc_Y50XT_v3vFSjM-_mX0U%3D&attredirects=0 [following]\n",
            "--2022-10-21 15:22:18--  https://ef80a887-a-62cb3a1a-s-sites.googlegroups.com/site/offensevalsharedtask/olid/OLIDv1.0.zip?attachauth=ANoY7cqEQR0eJ5TLdEsK4S1HzQw5bFGFDWy-0jht98Ph-6Q8mGDuWzno3ilDDXtWlo5Evm7FTuUwxwnaoPtcYIEaqxaqZj1BEJtqIPnfqS07Be3tEBGitu-JEuLBcEelaRVgKD6AQbJHTqClaV1TYKCyEd-vQmm7aDz66FGYfal_tS623Ld5T92DKYesJ21oZcOKVMBPe8rNZysr3OnhGI5A9LptINDjoZc_Y50XT_v3vFSjM-_mX0U%3D&attredirects=0\n",
            "Resolving ef80a887-a-62cb3a1a-s-sites.googlegroups.com (ef80a887-a-62cb3a1a-s-sites.googlegroups.com)... 172.217.212.137, 2607:f8b0:4001:c03::89\n",
            "Connecting to ef80a887-a-62cb3a1a-s-sites.googlegroups.com (ef80a887-a-62cb3a1a-s-sites.googlegroups.com)|172.217.212.137|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 874644 (854K) [application/zip]\n",
            "Saving to: ‘OLIDv1.0.zip’\n",
            "\n",
            "OLIDv1.0.zip        100%[===================>] 854.14K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-10-21 15:22:18 (57.2 MB/s) - ‘OLIDv1.0.zip’ saved [874644/874644]\n",
            "\n",
            "Archive:  OLIDv1.0.zip\n",
            "  inflating: olid-annotation.txt     \n",
            "  inflating: olid-training-v1.0.tsv  \n",
            "  inflating: README.txt              \n",
            "  inflating: labels-levela.csv       \n",
            "  inflating: labels-levelb.csv       \n",
            "  inflating: labels-levelc.csv       \n",
            "  inflating: testset-levelc.tsv      \n",
            "  inflating: testset-levelb.tsv      \n",
            "  inflating: testset-levela.tsv      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "data_train = []\n",
        "labels_train = []\n",
        "\n",
        "with open(\"olid-training-v1.0.tsv\") as f:\n",
        "    reader = csv.DictReader(f, delimiter=\"\\t\")\n",
        "    for row in reader:\n",
        "        words = [word.lower() for word in word_tokenize(row[\"tweet\"])]\n",
        "        data_train.append(words)\n",
        "        labels_train.append(row[\"subtask_a\"])\n",
        "\n",
        "data_test = []\n",
        "labels_test = []\n",
        "with open(\"testset-levela.tsv\") as f:\n",
        "    reader = csv.DictReader(f, delimiter=\"\\t\")\n",
        "    for row in reader:\n",
        "        words = [word.lower() for word in word_tokenize(row[\"tweet\"])]\n",
        "        data_test.append(words)\n",
        "\n",
        "with open(\"labels-levela.csv\") as f:\n",
        "    reader = csv.DictReader(f, fieldnames=[\"id\", \"label\"])\n",
        "    for row in reader:\n",
        "        labels_test.append(row[\"label\"])\n"
      ],
      "metadata": {
        "id": "yHd0sKA2XwyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "646cb9c6-f8e5-45c8-bf7a-77a2460e128b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use Keras' tokenizer only to compute the vocabulary on the training set. Sentences are truncated at 100 tokens and padding is added for shortes sentences."
      ],
      "metadata": {
        "id": "JaUGuWVzpN6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# transform the sentences into vectors\n",
        "tokenizer = Tokenizer(filters='', lower=True, split=' ')\n",
        "tokenizer.fit_on_texts(data_train)\n",
        "word_index = tokenizer.word_index\n",
        "X_train = tokenizer.texts_to_matrix(data_train)\n",
        "X_train = pad_sequences(X_train, 100, padding='post', truncating='post')\n",
        "\n",
        "# encode the labels\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(labels_train)\n",
        "y_train = encoder.transform(labels_train)\n",
        "\n",
        "# vectorize the test set\n",
        "X_test = tokenizer.texts_to_matrix(data_test)\n",
        "X_test = pad_sequences(X_test, 100, padding='post', truncating='post')\n",
        "y_test = encoder.transform(labels_test)\n"
      ],
      "metadata": {
        "id": "txQLn6TrVFjv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to initialize the weights of the first layer with pre-trained embeddings from GloVe."
      ],
      "metadata": {
        "id": "SZ_cfMoRqFS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "import numpy as np\n",
        "\n",
        "glove2word2vec(\"glove.6B.300d.txt\", \"glove_gensim.6B.300d.txt\")\n",
        "embedding_model=KeyedVectors.load_word2vec_format(\"glove_gensim.6B.300d.txt\",binary=False)"
      ],
      "metadata": {
        "id": "nPZIGPnZgDk4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2f746ba-eb34-48de-cff8-fb8aab8ded1e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-21 15:28:56--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-10-21 15:28:56--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
            "\n",
            "2022-10-21 15:31:35 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    try:\n",
        "        embedding_vector = embedding_model[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        continue\n"
      ],
      "metadata": {
        "id": "JhSDaCK7iMLV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The neural network has a first layer where the embeddings are input. They are then concatenated by the Flatten layer and passed on a smaller fully connected hidden layer. The output layer is one neuron with sigmoid activation for binary classification (offensive/not offensive)."
      ],
      "metadata": {
        "id": "Ga5Ph1p1pymp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embedding layer can be set to be trainable, or the weights can be kept frozen."
      ],
      "metadata": {
        "id": "yd48OgjuqPzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense, LSTM, Dropout, Activation, Bidirectional\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index)+1, 300, input_shape=(100,), weights=[embedding_matrix], trainable=True))\n",
        "model.add(Bidirectional(LSTM(16)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                  metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "        'balanced',classes=np.unique(y_train),y= y_train)\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                        batch_size=16,\n",
        "                        epochs=5,\n",
        "                        shuffle=True,\n",
        "                        validation_split=0.1,\n",
        "                        class_weight=class_weights,\n",
        "                        verbose=1\n",
        "                        )"
      ],
      "metadata": {
        "id": "hxv1cnNIiOm9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9285c740-5d35-406c-e9af-2477f96e1c0e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 300)          6444300   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 32)               40576     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,484,909\n",
            "Trainable params: 6,484,909\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "745/745 [==============================] - 20s 15ms/step - loss: 0.6932 - accuracy: 0.4988 - val_loss: 0.6868 - val_accuracy: 0.6156\n",
            "Epoch 2/5\n",
            "745/745 [==============================] - 10s 14ms/step - loss: 0.6927 - accuracy: 0.5120 - val_loss: 0.6863 - val_accuracy: 0.5793\n",
            "Epoch 3/5\n",
            "745/745 [==============================] - 10s 14ms/step - loss: 0.6913 - accuracy: 0.5264 - val_loss: 0.6908 - val_accuracy: 0.5249\n",
            "Epoch 4/5\n",
            "745/745 [==============================] - 10s 14ms/step - loss: 0.6914 - accuracy: 0.5376 - val_loss: 0.6946 - val_accuracy: 0.5045\n",
            "Epoch 5/5\n",
            "745/745 [==============================] - 10s 14ms/step - loss: 0.6909 - accuracy: 0.5372 - val_loss: 0.6871 - val_accuracy: 0.5438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "pred = [int(x>=0.5) for x in model.predict(X_test)]\n",
        "print (classification_report(y_test, pred))"
      ],
      "metadata": {
        "id": "v1CVwCKnjfgQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01ec3e88-0ba3-4d6a-8af2-9657b6ef87a9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27/27 [==============================] - 1s 6ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.73      0.72       620\n",
            "           1       0.23      0.21      0.22       240\n",
            "\n",
            "    accuracy                           0.58       860\n",
            "   macro avg       0.47      0.47      0.47       860\n",
            "weighted avg       0.57      0.58      0.58       860\n",
            "\n"
          ]
        }
      ]
    }
  ]
}