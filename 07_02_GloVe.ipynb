{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will load a pre-trained word embedding model and explore it with [Gensim](https://radimrehurek.com/gensim/).\n",
        "\n",
        "The word embeddings are created with GloVe and are available on the [project website](https://nlp.stanford.edu/projects/glove/). In partiular, we will use the model trained on Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download)."
      ],
      "metadata": {
        "id": "9H_YbB235w6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "id": "jnJ5LuUcf6i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The archive contains one file per model, with the only difference being the number of dimensions. Each line is made of a token followed by whitespace-separated numbers (the n-dimensional vector components)."
      ],
      "metadata": {
        "id": "CGOoABZNiywG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n1 glove.6B.300d.txt"
      ],
      "metadata": {
        "id": "PiOdBLPwishu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The format understood by Gensim is the same as the default Word2vec format, where the first line of the file must contain the number of lines and the number of dimensions. This utility function adds the first line to the GloVe embedding file. "
      ],
      "metadata": {
        "id": "PnhXkUa86lSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "glove2word2vec(\"glove.6B.300d.txt\", \"glove_gensim.6B.300d.txt\")"
      ],
      "metadata": {
        "id": "mOtJgXkLQsiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head glove_gensim.6B.300d.txt"
      ],
      "metadata": {
        "id": "nxooRAIDwA0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embeddings are loaded into a KeyedVectors object, basically a dictionary with words as keys ad vectors as values."
      ],
      "metadata": {
        "id": "gzsOXuEBv--9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=KeyedVectors.load_word2vec_format(\"glove_gensim.6B.300d.txt\",binary=False)"
      ],
      "metadata": {
        "id": "LbAllLWWv8V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the embeddings are loaded, the model object can be queried directly as a dictionary (but it retains the other functions too)."
      ],
      "metadata": {
        "id": "C337C9Ju7v92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (model['rock'])"
      ],
      "metadata": {
        "id": "MSeryQA97v0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the N words most similar to an input word, or calculating word pair similarity, is the same that with Word2vec."
      ],
      "metadata": {
        "id": "-932o40U7vfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word, similarity in model.most_similar('school', topn=10):\n",
        "    print (f\"{similarity:.2f} {word}\")"
      ],
      "metadata": {
        "id": "pXdBxZk_RGA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe is known for its ability to perform analogical (as in *analogy*) reasoning. With Gensim, analogies are implemented with simple geometric operations. For an analogy of the form: \n",
        "\n",
        "    A:B = C:?\n",
        "\n",
        "we can look for vectors similar to B and C, and at the same time dissimilar from A."
      ],
      "metadata": {
        "id": "ZoWcHL9BShWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paris is to France as Berlin is to ...\n",
        "print (model.most_similar(positive=[model['france'], model['berlin']], negative=[model['paris']])[0][0])\n",
        "\n",
        "# Man is to actor as woman is to ...\n",
        "print (model.most_similar(positive=[model['actor'], model['woman']], negative=[model['man']])[0][0])"
      ],
      "metadata": {
        "id": "If-LCjJd0gi6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}